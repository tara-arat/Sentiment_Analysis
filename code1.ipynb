{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\keert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\keert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Reviewer Name               Review Title  \\\n",
      "0            Kamal Suresh               Nice product   \n",
      "1       Flipkart Customer     Don't waste your money   \n",
      "2  A. S. Raja Srinivasan   Did not meet expectations   \n",
      "3     Suresh Narayanasamy                       Fair   \n",
      "4               ASHIK P A                Over priced   \n",
      "\n",
      "               Place of Review  Up Votes  Down Votes     Month  \\\n",
      "0   Certified Buyer, Chirakkal     889.0        64.0  Feb 2021   \n",
      "1   Certified Buyer, Hyderabad     109.0         6.0  Feb 2021   \n",
      "2  Certified Buyer, Dharmapuri      42.0         3.0  Apr 2021   \n",
      "3     Certified Buyer, Chennai      25.0         1.0       NaN   \n",
      "4                          NaN     147.0        24.0  Apr 2016   \n",
      "\n",
      "                                         Review text  Ratings  \\\n",
      "0  Nice product, good quality, but price is now r...        4   \n",
      "1  They didn't supplied Yonex Mavis 350. Outside ...        1   \n",
      "2  Worst product. Damaged shuttlecocks packed in ...        1   \n",
      "3  Quite O. K. , but nowadays  the quality of the...        3   \n",
      "4  Over pricedJust â?¹620 ..from retailer.I didn'...        1   \n",
      "\n",
      "                                        Cleaned Text  Sentiment  \n",
      "0  nice product good quality price rising bad sig...          1  \n",
      "1  didnt supplied yonex mavis outside cover yonex...          0  \n",
      "2  worst product damaged shuttlecock packed new b...          0  \n",
      "3  quite k nowadays quality cork like year back u...          1  \n",
      "4  pricedjust retaileri didnt understand wat adva...          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\keert\\Downloads\\reviews_badminton\\data.csv\")\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Check if text is not NaN\n",
    "    if pd.notnull(text):\n",
    "        # Convert text to lowercase\n",
    "        text = str(text).lower()  # Convert to string before applying lower() method\n",
    "        \n",
    "        # Remove special characters and punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Join the tokens back into a single string\n",
    "        clean_text = ' '.join(tokens)\n",
    "        \n",
    "        return clean_text\n",
    "    else:\n",
    "        return ''  # Return empty string for NaN values\n",
    "\n",
    "# Apply text preprocessing to 'Review text' column\n",
    "df['Cleaned Text'] = df['Review text'].apply(preprocess_text)\n",
    "\n",
    "# Define threshold for positive sentiment\n",
    "threshold = 3\n",
    "\n",
    "# Create 'Sentiment' column based on 'Ratings'\n",
    "df['Sentiment'] = df['Ratings'].apply(lambda x: 1 if x >= threshold else 0)\n",
    "\n",
    "# Check the first few rows of the DataFrame to verify the 'Sentiment' column\n",
    "print(df.head())\n",
    "\n",
    "# Now, you can proceed with the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Cleaned Text'], df['Sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Cleaned Text'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models and pipelines\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Support Vector Machine', SVC())\n",
    "]\n",
    "\n",
    "# Pipeline for numerical feature extraction and model training\n",
    "pipelines = []\n",
    "for model_name, model in models:\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    pipelines.append((model_name, pipeline))\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for model_name, pipeline in pipelines:\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    results[model_name] = f1\n",
    "\n",
    "# Find best model\n",
    "best_model = max(results, key=results.get)\n",
    "print(\"Best Model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores of all models:\n",
      "Random Forest: 0.8976280869047202\n",
      "Logistic Regression: 0.9065500042218775\n",
      "Support Vector Machine: 0.901545417616285\n",
      "Best Model: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for model_name, pipeline in pipelines:\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    results[model_name] = f1\n",
    "\n",
    "# Print F1-scores of all models\n",
    "print(\"F1-scores of all models:\")\n",
    "for model_name, f1 in results.items():\n",
    "    print(f\"{model_name}: {f1}\")\n",
    "\n",
    "# Find best model\n",
    "best_model = max(results, key=results.get)\n",
    "print(\"Best Model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9196009389671361\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.39      0.53       199\n",
      "           1       0.92      0.99      0.96      1505\n",
      "\n",
      "    accuracy                           0.92      1704\n",
      "   macro avg       0.88      0.69      0.74      1704\n",
      "weighted avg       0.91      0.92      0.91      1704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logistic_regression_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Predict the sentiment labels for test data\n",
    "y_pred = logistic_regression_model.predict(X_test_tfidf)\n",
    "\n",
    "# Save the trained model and TF-IDF vectorizer\n",
    "joblib.dump(logistic_regression_model, \"logistic_regression_model.pkl\")\n",
    "joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
